---
title: "R Notebook"
output: html_notebook
---

define used libraries
```{r}
libraries_used <- c("lazyeval", "readr","plyr" ,"dplyr", "readxl", "ggplot2", 
    "funModeling", "scales", "tidyverse", "corrplot", "GGally", "caret",
    "rpart", "randomForest", "pROC", "gbm", "choroplethr", "choroplethrMaps",
    "microbenchmark", "doParallel", "e1071")
```

check missing libraries and install them

```{r}
libraries_missing <- libraries_used[!(libraries_used %in% installed.packages()[,"Package"])]
if(length(libraries_missing)) install.packages(libraries_missing)
```

use dplyr
```{r}
library(dplyr)
```

load dataset
```{r}
df <- read.csv("regression_train_loan.csv")
```

get unique values of loan_status
```{r}
unique( df$loan_status )
```


get meta information and output it
```{r}
meta_df <- funModeling::df_status( df, print_results = FALSE )

knitr::kable(meta_df)
```

get amount and percentage of unique and output the table
```{r}
meta_df_p <- meta_df %>% mutate(uniq_rat = unique / nrow(df))
meta_df_p %>% select(variable, unique, uniq_rat) %>% mutate(unique = unique, uniq_rat = scales::percent(uniq_rat)) %>% knitr::kable()
```

convert date attributes
```{r}
chr_to_date_vars <-  c("issue_d", "last_pymnt_d", "last_credit_pull_d", "next_pymnt_d", "earliest_cr_line", "next_pymnt_d")
convert_date <- function(x){
  as.Date(paste0("01-", x), format = "%d-%b-%Y")
}

df <- df %>% mutate_at(.funs = funs(convert_date), .vars = chr_to_date_vars)
```

output a table with amount of unique, N/A, and zero values
```{r}
meta_df %>% select(variable, p_zeros, p_na, unique) %>% knitr::kable()
```

recreate meta table
```{r}
meta_df <- funModeling::df_status( df, print_results = FALSE )
```

set vars from N/A to zero
```{r}
na_to_zero_vars <- c("mths_since_last_delinq", "mths_since_last_record", "mths_since_last_major_derog")

df <- df %>% mutate_at(.vars = na_to_zero_vars, .funs = funs(replace(., is.na(.), 0)))
```

remove columns deemed unfit for modeling. All columns with more than 70% missing values are deemed to be unhelpful in modelling and exploration.
```{r}
vars_to_remove <- c("annual_inc_joint", "dti_joint", "policy_code", "id", "member_id",
                    "emp_title", "url", "desc", "title", "open_acc_6m", "open_il_6m", 
                    "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", 
                    "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util",
                    "total_rev_hi_lim", "inq_fi", "total_cu_tl", "inq_last_12m",
                    "verification_status_joint", "next_pymnt_d", "sub_grade", "X", "zip_code")

df_clean <- df %>% select(-one_of(vars_to_remove))
```

set the seed for reproductionability
```{r}
set.seed(1)
```

set the training and test data (80% train, 20% test)
```{r}
train_index <- caret::createDataPartition(y = df_clean$int_rate, times = 1, p = .8, list = FALSE)

train <- df_clean[train_index, ]
test <- df_clean[-train_index, ]
```

Output numerical vars
```{r}
num_vars <- 
  train %>% 
  sapply(is.numeric) %>% 
  which() %>% 
  names()

meta_train <- funModeling::df_status(train, print_results = FALSE)

meta_train %>%
  select(variable, p_zeros, p_na, unique) %>%
  filter_(~ variable %in% num_vars) %>%
  knitr::kable()

```

Correlation plot. First step to get an overview of potential collinearities. Failing to identify multicollinearity could result in misleading interpretations of the results.
```{r}
library(corrplot)
corrplot::corrplot(cor(train[, num_vars], use = "complete.obs"), 
                   method = "pie", type = "upper",tl.cex = 0.45)
```
Display highly correlated variables. We want to identify collinearities which could affect our regression model. A Cut-off at 0.8 is applied as suggested by Berry & Feldman, 1985: Multiple Regression in Practice (Quantitative Applications in the Social Sciences)

```{r}
caret::findCorrelation(cor(train[, num_vars], use = "complete.obs"), 
                       names = TRUE, cutoff = .8)
```

Remove highly correlated variables to avoid the interference of collinearity in our model.  Multicollinearity arises when at least two highly correlated predictors are assessed simultaneously in a regression model.
```{r}
vars_to_remove <- 
  c("loan_amnt", "funded_amnt", "funded_amnt_inv", "installment",
    "total_pymnt_inv", "out_prncp", "collection_recovery_fee")

train <- train %>% select(-one_of(vars_to_remove))

```

Set proper names for variables
```{r}
vars_to_mutate <-
  train %>%
  select(which(sapply(.,is.character))) %>%
  names()

vars_to_mutate

test <-
  test %>%
  mutate_at(.funs = make.names, .vars = vars_to_mutate)
```
use caret library
```{r}
library(caret)
```

Assignment 1: Regression Model for predicting int_rate.

Create model for a logistic regression
```{r}
attach(test)

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

model_logreg <- train(int_rate ~ ., data = train, 
                 method = "bayesglm",
                 trControl = fitControl,
                 na.action=na.exclude)

class(model_logreg)

summary (model_logreg)
```

Train multiple linear regression model
```{r}
model_lm <- train(int_rate ~ ., data = train, 
                 method = "lm",
                 trControl = fitControl,
                 na.action=na.exclude)

class(model_lm)

summary (model_lm)
```


```{r}
model_logreg_pred_1 <- 
  predict.glm(object = model_logreg, newdata = test, type = "response")
model_pred_t <- function(pred, t) ifelse(pred > t, TRUE, FALSE)
caret::confusionMatrix(data = model_pred_t(model_logreg_pred_1, 0.5), 
                       reference = test$default,
                       positive = "TRUE")
```



Interpret Model performance using ROC
```{r}
library(pROC)

roc_model_logreg <- pROC::roc(response = int_rate, predictor = model_logreg)
roc_model_logreg

pROC::plot.roc(x = roc_model_logreg, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               col = "green", print.auc = FALSE, print.auc.y = .4)

legend(x = "bottomright", col = c("green"), lty = 1, cex = 1.0)
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
