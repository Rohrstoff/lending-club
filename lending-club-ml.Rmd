---
title: "Assignment 1: Machine Learning Model"
output: html_notebook
---

<h1>Lessons learned</h1>
<p>The assignments gave us the opportunity to apply the methods we got to know during the lectures on real-world data. Working with GitHub for exchanging development progresses and Microsoft Teams for virtual meetups proved to be valuable, even more so under the special circumstances present today. As expected, the data preparation took at least as long as developing the models. Although we applied the methods learned during the courses, we found further RStudio functions that vastly assisted in pre-processing of the data. Without proper pre-processing, especially conversion of date and numeric attributes to their respective data type, any model development is futile. We overlooked some attributes in the beginning which led to weird predictions. Excluding near-zero variance predictors was a step we only learned to apply due to the warning messages of trained models. In the AI assignment we learned to not apply too many hidden layers (we sticked with 2) and 16 neurons per hidden layer are sufficient, in the context of binary cross-entropy.</p>

<p>Overall we found libraries during our research that massively helped us in the pre-processing step, such as knitr, and later on recipes. We had difficulties to run models with high numbers of cross-validations, which is something that should be kept in mind for the future. Overall, it was interesting to have a complete perspective on data science process, however we did not feel ready to tackle the assignment purely with the inputs from the lectures. Extensive research was required to properly understand all required pre-processing steps.</p>

<h3>Define, install and import used libraries</h3>
```{r}
libraries_used <- c("lazyeval", "readr","plyr" ,"dplyr", "readxl", "ggplot2", 
    "funModeling", "scales", "tidyverse", "corrplot", "GGally", "caret",
    "rpart", "randomForest", "gbm", "choroplethr", "choroplethrMaps",
    "microbenchmark", "doParallel", "e1071")

libraries_missing <- libraries_used[!(libraries_used %in% installed.packages()[,"Package"])]
if(length(libraries_missing)) install.packages(libraries_missing)

library(dplyr)
library(recipes)
library(caret)
library(corrplot)

set.seed(1)
```

<h3>Load Dataset</h3>
```{r}
df <- read.csv("regression_train_loan.csv")
```

<h3>Extract meta information and output it</h3>
```{r}
meta_df <- funModeling::df_status( df, print_results = FALSE )

knitr::kable(meta_df)
```

<h3>Get amount and percentage of unique</h3>
```{r}
meta_df_p <- meta_df %>% mutate(uniq_rat = unique / nrow(df))
meta_df_p %>% select(variable, unique, uniq_rat) %>% mutate(unique = unique, uniq_rat = scales::percent(uniq_rat)) %>% knitr::kable()
```

<h3>Convert Date attributes</h3>
```{r}
chr_to_date_vars <-  c("issue_d", "last_pymnt_d", "last_credit_pull_d", "next_pymnt_d", "earliest_cr_line", "next_pymnt_d")
convert_date <- function(x){
  as.Date(paste0("01-", x), format = "%d-%b-%Y")
}

df <- df %>% mutate_at(.funs = funs(convert_date), .vars = chr_to_date_vars)
```

<h3>Convert Numeric attributes</h3>
```{r}
chr_to_num_vars <- 
  c("annual_inc_joint", "mths_since_last_major_derog", "open_acc_6m",
    "open_il_6m", "open_il_12m", "open_il_24m", "mths_since_rcnt_il",
    "total_bal_il", "il_util", "open_rv_12m", "open_rv_24m",
    "max_bal_bc", "all_util", "total_rev_hi_lim", "total_cu_tl",
    "inq_last_12m", "dti_joint", "inq_fi", "tot_cur_bal", "tot_coll_amt", "int_rate")

df <- df %>%  mutate_at(.funs = funs(as.numeric), .vars = chr_to_num_vars)
```

<h3>Output a table with amount of unique, N/A, and zero values</h3>
```{r}
meta_df %>% select(variable, p_zeros, p_na, unique) %>% knitr::kable()
```

<h3>Remove columns deemed unfit for modeling</h3>
<p>All columns with more than 70% missing values are deemed to be unhelpful in modeling and exploration, as well as certain features</p>
```{r}
vars_to_remove <- c("annual_inc_joint", "dti_joint", "policy_code", "id", "member_id",
                    "emp_title", "url", "desc", "title", "open_acc_6m", "open_il_6m", 
                    "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", 
                    "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util",
                    "total_rev_hi_lim", "inq_fi", "total_cu_tl", "inq_last_12m",
                    "verification_status_joint", "next_pymnt_d", "sub_grade", "X", "zip_code", "application_type",
                    "mths_since_last_major_derog", "last_pymnt_d", "last_credit_pull_d", "issue_d", "delinq_2yrs", "inq_last_6mths", "total_rec_int",
                    "total_rec_late_fee", "last_pymnt_amnt")

df <- df %>% select(-one_of(vars_to_remove))
```

<h3>Convert "term" column to number</h3>
```{r}
df$term <- as.numeric(gsub( ".*([0-9]+).*", "\\1", df$term ))
```

<h3>Convert "emp_length" to numeric values</h3>
```{r}
df$emp_length[df$emp_length == "n/a"] <- 99
df$emp_length[df$emp_length == "< 1 year"] <- 0
df$emp_length <- as.numeric(gsub( "([0-9]+).*", "\\1", df$emp_length ))
```

<h3>Output numerical vars</h3>
```{r}
num_vars <- 
  df %>% 
  sapply(is.numeric) %>% 
  which() %>% 
  names()

meta_df <- funModeling::df_status(df, print_results = FALSE)

meta_df %>%
  select(variable, p_zeros, p_na, unique) %>%
  filter_(~ variable %in% num_vars) %>%
  knitr::kable()

```

<h3>Correlation plot</h3> 
<p>First step to get an overview of potential collinearities. Failing to identify multicollinearity could result in misleading interpretations of the results.</p>
```{r}
corrplot::corrplot(cor(df[, num_vars], use = "complete.obs"), 
                   method = "pie", type = "upper",tl.cex = 0.65)
```

<h3>Display highly correlated variables</h3>
<p>We want to identify collinearities which could affect our regression model. A Cut-off at 0.5 is applied.</p>

```{r}
caret::findCorrelation(cor(df[, num_vars], use = "complete.obs"), 
                       names = TRUE, cutoff = .5)
```

<h3>Remove highly correlated variables to avoid the interference of collinearity in our model</h3>
<p>Multicollinearity arises when at least two highly correlated predictors are assessed simultaneously in a regression model.</p>
```{r}
vars_to_remove <- 
  c("loan_amnt", "funded_amnt", "funded_amnt_inv", "installment", "total_pymnt_inv", 
    "out_prncp", "total_pymnt", "total_rec_prncp", "total_acc",
    "mths_since_last_record",  "mths_since_last_delinq", "recoveries")

df <- df %>% select(-one_of(vars_to_remove))

```

<h3>Remove rows where date columns contain N/A values (25 recs)</h3>
```{r}
df <- df[complete.cases(df), ]
```

<h3>Recreate meta table</h3>
```{r}
meta_df <- funModeling::df_status(df, print_results = FALSE)
knitr::kable( meta_df )
```

<h3>Set the training and test data (80% train, 20% test)</h3>
```{r}
train_index <- caret::createDataPartition(y = df$int_rate, times = 1, p = .8, list = FALSE)

train <- df[train_index, ]
test <- df[-train_index, ]
```


<h3>Data preprocessing</h3>

<p>
The following steps are executed:

<ul>
<li>Remove near-zero variance predictors. These predictors can interfere with our model training using cross-validations. We want to remove variables with a high frequency ratio and a high percentage of unique values.</li>

<li>Convert date predictors into factors/numeric variables and remove the date predictors afterwards</li>

<li>Create dummy variables of nominal predictors.</li>

<li>Normalize numeric data to have a standard deviation of one and a mean of zero</li>
</ul>
</p>
```{r}
recipe_obj <- recipe( int_rate ~ ., data = train ) %>%
  step_nzv(all_predictors()) %>%
  step_date( has_type( match = "date" ) ) %>%
  step_rm( has_type( match = "date" ) ) %>%
  step_dummy( all_nominal(), -all_outcomes() ) %>%
  step_normalize( all_predictors() )

recipe_obj
```

<h3>Define the resampling method for model training</h3>
<p>We apply a 10-time cross validation for our regression models.</p>
```{r}
fitControl <- trainControl(method = "cv", verboseIter = TRUE)
```

<h3>Train linear regression model</h3>
```{r}
model <- train(recipe_obj, data = train,
               method = "lm",
               tuneLength = 10,
               trControl = fitControl)

summary(model)
```

<h3>Generate predictions and display RMSE, Rsquared and MAE</h3>
```{r}
test_results <- data.frame(Class = test$int_rate)
test_results$predicted.int_rate <- predict(model, test)

postResample(pred = test_results$predicted.int_rate, obs = test$int_rate)
```

<h3>Show the test set results</h3>
```{r}
head(test_results)
```

<h3>Save the required data to the filesystem</h3>
```{r}
saveRDS(model_ml_lm, "model.lm")
write.csv(train_filtered, "ml_train.csv", row.names = TRUE)
write.csv(test, "ml_test.csv", row.names = TRUE)
```