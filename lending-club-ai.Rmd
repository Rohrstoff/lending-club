---
title: "Lending Club Neural Network"
output:
  html_document:
    df_print: paged
---

Prepare the correct language settings for the R-Environment. E.g. german console output may lead to issues. Default should be english.

```{r}
# Sys.setlocale("LC_ALL","English")
# Sys.setenv(LANG = "en_US.UTF-8")
# Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
# 
# ## Check if the settings are correct. It should look like:
# ## [1] "LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252"
# Sys.getlocale()
# 
```


Install and Load Libraries

```{r}
libraries_used <- c("keras", "lime", "tidyquant", "rsample", "recipes", "yardstick", "corrr", "caret")

libraries_missing <- libraries_used[!(libraries_used %in% installed.packages()[,"Package"])]
if(length(libraries_missing)) install.packages(libraries_missing)

# Load libraries
library(keras)
library(lime)
library(tidyquant)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)
library(caret)

set.seed(1)
```

load dataset
```{r}
train_frame <- read.csv("regression_train_loan.csv")
test_frame <- read.csv("loan_eval.csv")
```

get meta information and output it
```{r}
meta_df <- funModeling::df_status( train_frame, print_results = FALSE )

knitr::kable(meta_df)
```

get amount and percentage of unique and output the table
```{r}
meta_df_percent <- meta_df %>% mutate(uniq_rat = unique / nrow(train_frame))
meta_df_percent %>% select(variable, unique, uniq_rat) %>% mutate(unique = unique, uniq_rat = scales::percent(uniq_rat)) %>% knitr::kable()
```

output a table with amount of unique, N/A, and zero values
```{r}
meta_df %>% select(variable, p_zeros, p_na, unique) %>% knitr::kable()
```

remove columns deemed unfit for modeling
```{r}
vars_to_remove <- c("annual_inc_joint", "dti_joint", "policy_code", "id", "member_id",
                    "emp_title", "url", "desc", "title", "open_acc_6m", "open_il_6m", 
                    "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", 
                    "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util",
                    "total_rev_hi_lim", "inq_fi", "total_cu_tl", "inq_last_12m",
                    "verification_status_joint", "next_pymnt_d", "sub_grade", "X", "zip_code", "application_type")

train_frame <- train_frame %>% select(-one_of(vars_to_remove))
test_frame <- test_frame %>% select(-one_of(vars_to_remove))
```

convert date attributes
```{r}
chr_to_date_vars <-  c("issue_d", "last_pymnt_d", "last_credit_pull_d", "earliest_cr_line")
convert_date <- function(x){
  as.Date(paste0("01-", x), format = "%d-%b-%Y")
}

train_frame <- train_frame %>% mutate_at(.funs = list(convert_date), .vars = chr_to_date_vars)
test_frame <- test_frame %>% mutate_at(.funs = list(convert_date), .vars = chr_to_date_vars)
```

Output numerical vars
```{r}
num_vars <-  train_frame %>%  sapply(is.numeric) %>%  which() %>% names()

meta_df <- funModeling::df_status(train_frame, print_results = FALSE) %>%
  select(variable, p_zeros, p_na, unique) %>%
  filter_(~ variable %in% num_vars) %>%
  knitr::kable()

```

Correlation plot. First step to get an overview of potential collinearities. Failing to identify multicollinearity could result in misleading interpretations of the results.
```{r}
corrplot::corrplot(cor(train_frame[, num_vars], use = "complete.obs"), 
                   method = "pie", type = "upper",tl.cex = 0.45)
```
```{r}
meta_df <- funModeling::df_status( train_frame, print_results = FALSE )
meta_df %>% select(variable, p_zeros, p_na, unique) %>% knitr::kable()
```

Display highly correlated variables. We want to identify collinearities which could affect our regression model. A Cut-off at 0.8 is applied as suggest by Berry & Feldman, 1985: Multiple Regression in Practice (Quantitative Applications in the Social Sciences)

```{r}
caret::findCorrelation(cor(train_frame[, num_vars], use = "complete.obs"), names = TRUE, cutoff = .8)
```

Remove highly correlated variables to avoid the interference of collinearity in our model.  Multicollinearity arises when at least two highly correlated predictors are assessed simultaneously in a regression model.
```{r}
vars_to_remove <- 
  c("loan_amnt", "funded_amnt", "funded_amnt_inv", "total_pymnt",
    "total_pymnt_inv", "out_prncp", "collection_recovery_fee")

train_frame <- train_frame %>% select(-one_of(vars_to_remove))
test_frame <- test_frame %>% select(-one_of(vars_to_remove))

```

recreate meta table
```{r}
meta_df <- funModeling::df_status( train_frame, print_results = FALSE )
meta_df %>% select(variable, p_zeros, p_na, unique) %>% knitr::kable()
```

set vars from N/A to zero
```{r}
na_to_zero_vars <- c("mths_since_last_delinq", "mths_since_last_record", "mths_since_last_major_derog")

train_frame <- train_frame %>% mutate_at(.vars = na_to_zero_vars, .funs = funs(replace(., is.na(.), 0)))
test_frame <- test_frame %>% mutate_at(.vars = na_to_zero_vars, .funs = funs(replace(., is.na(.), 0)))
```

set vars from N/A to -99
```{r}
na_to_negative_ninety_nine <- c("revol_util", "collections_12_mths_ex_med", "tot_coll_amt", "tot_cur_bal")

train_frame <- train_frame %>% mutate_at(.vars = na_to_negative_ninety_nine, .funs = funs(replace(., is.na(.), -99)))
test_frame <- test_frame %>% mutate_at(.vars = na_to_negative_ninety_nine, .funs = funs(replace(., is.na(.), -99)))
```

Remove rows where date columns contain N/A values
```{r}
train_frame <- train_frame[complete.cases(train_frame), ]
test_frame <- test_frame[complete.cases(test_frame), ]
```

Remove all rows where loan status is "Current" + rename all loan_status fields where the value is not "Fully Paid"
```{r}
train_frame <- subset(train_frame, loan_status != "Current") 
train_frame$loan_status[train_frame$loan_status != "Fully Paid"] <- "DEFAULTED"

test_frame <- subset(test_frame, loan_status != "Current") 
test_frame$loan_status[test_frame$loan_status != "Fully Paid"] <- "DEFAULTED"
```

Convert "term" column to number
```{r}
train_frame$term <- as.numeric(gsub( ".*([0-9]+).*", "\\1", train_frame$term ))

test_frame$term <- as.numeric(gsub( ".*([0-9]+).*", "\\1", test_frame$term ))
```

Convert "emp_length" to numeric values
```{r}
train_frame$emp_length[train_frame$emp_length == "n/a"] <- 99
train_frame$emp_length[train_frame$emp_length == "< 1 year"] <- 0
train_frame$emp_length <- as.numeric(gsub( "([0-9]+).*", "\\1", train_frame$emp_length ))

test_frame$emp_length[test_frame$emp_length == "n/a"] <- 99
test_frame$emp_length[test_frame$emp_length == "< 1 year"] <- 0
test_frame$emp_length <- as.numeric(gsub( "([0-9]+).*", "\\1", test_frame$emp_length ))
```

Remove near-zero variance predictors. These predictors can interfere with our model training using cross-validations. We want to remove variables with a high frequency ratio and a high percentage of unique values.
```{r}
recipe_obj <- recipe( loan_status ~ ., data = train_frame ) %>%
  step_nzv(all_predictors()) %>%
  step_date( has_type( match = "date" ) ) %>%
  step_rm( has_type( match = "date" ) ) %>%
  step_dummy( all_nominal(), -all_outcomes() ) %>%
  step_normalize( all_numeric(), -all_outcomes() ) %>%
  step_scale( all_numeric(), -all_outcomes() ) %>%
  step_center(all_predictors()) %>%
  prep()

x_train_tbl <- bake(recipe_obj, new_data = train_frame) %>% select(-loan_status)
x_test_tbl <- bake(recipe_obj, new_data = test_frame) %>% select(-loan_status)

y_train_vec <- ifelse(pull(train_frame, loan_status) == "DEFAULTED", 1, 0)
y_test_vec <- ifelse(pull(test_frame, loan_status) == "DEFAULTED", 1, 0)
```


```{r}
# Building our Artificial Neural Network
model_keras <- keras_model_sequential() %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, 
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile ANN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

model_keras 
```


```{r}
# Fit the keras model to the training data
history <- fit(
  object           = model_keras, 
  x                = as.matrix(x_train_tbl), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30
)

print(history)
```

```{r}
plot(history) 
```

Estimates
```{r}
# Predicted Class
yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%
    as.vector()

# Predicted Class Probability
yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%
    as.vector()

# Format test data and predictions for yardstick metrics
estimates_keras_tbl <- tibble(
  truth      = as.factor(y_test_vec) %>% forcats::fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec) %>% forcats::fct_recode(yes = "1", no = "0"),
  class_prob = yhat_keras_prob_vec
)

estimates_keras_tbl
```

Confusion Matrix
```{r}
options(yardstick.event_first = FALSE)

estimates_keras_tbl %>% conf_mat(truth, estimate)
```

Accuracy
```{r}
estimates_keras_tbl %>% metrics(truth, estimate)
```

AUC
```{r}
estimates_keras_tbl %>% roc_auc(truth, class_prob)
```

Precision
```{r}
tibble(
  precision = estimates_keras_tbl %>% precision(truth, estimate),
  recall    = estimates_keras_tbl %>% recall(truth, estimate)
)
```

F1-Statistic
```{r}
estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
```


Export the required data to the filesystem
```{r}
keras::save_model_hdf5(model_keras, "network.nn")
write.csv(x_train_tbl, "nn_train.csv", row.names = TRUE)
write.csv(x_test_tbl, "nn_test.csv", row.names = TRUE)
```